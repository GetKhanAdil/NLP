{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae22c7b",
   "metadata": {},
   "source": [
    "## Importing the Necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d67a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417e6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= '''Text preprocessing is an important role when it comes to Natural Language Processing. A text will always be in the natural human format of sentences, paragraphs. Therefore before performing any further analysis, the text has to be cleaned and break the text down into a format the computer can easily understand.\n",
    "Tokenization is about splitting strings of text into smaller pieces, or “tokens”. Paragraphs can be tokenized into sentences and sentences can be tokenized into words. Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens, perhaps at the same time throwing away certain characters, such as punctuation.\n",
    "Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units is called tokens.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a98eb",
   "metadata": {},
   "source": [
    "# We tokenize the the paragraph into sentences and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0183c571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokenize \n",
      "\n",
      "['Text preprocessing is an important role when it comes to Natural Language Processing.', 'A text will always be in the natural human format of sentences, paragraphs.', 'Therefore before performing any further analysis, the text has to be cleaned and break the text down into a format the computer can easily understand.', 'Tokenization is about splitting strings of text into smaller pieces, or “tokens”.', 'Paragraphs can be tokenized into sentences and sentences can be tokenized into words.', 'Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens, perhaps at the same time throwing away certain characters, such as punctuation.', 'Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms.', 'Each of these smaller units is called tokens.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence tokenize \\n\")\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9742dadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokenize \n",
      "\n",
      "['Text', 'preprocessing', 'is', 'an', 'important', 'role', 'when', 'it', 'comes', 'to', 'Natural', 'Language', 'Processing', '.', 'A', 'text', 'will', 'always', 'be', 'in', 'the', 'natural', 'human', 'format', 'of', 'sentences', ',', 'paragraphs', '.', 'Therefore', 'before', 'performing', 'any', 'further', 'analysis', ',', 'the', 'text', 'has', 'to', 'be', 'cleaned', 'and', 'break', 'the', 'text', 'down', 'into', 'a', 'format', 'the', 'computer', 'can', 'easily', 'understand', '.', 'Tokenization', 'is', 'about', 'splitting', 'strings', 'of', 'text', 'into', 'smaller', 'pieces', ',', 'or', '“', 'tokens', '”', '.', 'Paragraphs', 'can', 'be', 'tokenized', 'into', 'sentences', 'and', 'sentences', 'can', 'be', 'tokenized', 'into', 'words', '.', 'Given', 'a', 'character', 'sequence', 'and', 'a', 'defined', 'document', 'unit', ',', 'tokenization', 'is', 'the', 'task', 'of', 'chopping', 'it', 'up', 'into', 'pieces', ',', 'called', 'tokens', ',', 'perhaps', 'at', 'the', 'same', 'time', 'throwing', 'away', 'certain', 'characters', ',', 'such', 'as', 'punctuation', '.', 'Tokenization', 'is', 'essentially', 'splitting', 'a', 'phrase', ',', 'sentence', ',', 'paragraph', ',', 'or', 'an', 'entire', 'text', 'document', 'into', 'smaller', 'units', ',', 'such', 'as', 'individual', 'words', 'or', 'terms', '.', 'Each', 'of', 'these', 'smaller', 'units', 'is', 'called', 'tokens', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Word tokenize \\n\")\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b28aa",
   "metadata": {},
   "source": [
    "## Now we are using stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e7042b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb9e0ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a52f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6ea985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaf297a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_filteration=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59961723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after filteration \n",
      "\n",
      " ['Text', 'preprocessing', 'important', 'role', 'comes', 'Natural', 'Language', 'Processing', '.', 'A', 'text', 'always', 'natural', 'human', 'format', 'sentences', ',', 'paragraphs', '.', 'Therefore', 'performing', 'analysis', ',', 'text', 'cleaned', 'break', 'text', 'format', 'computer', 'easily', 'understand', '.', 'Tokenization', 'splitting', 'strings', 'text', 'smaller', 'pieces', ',', '“', 'tokens', '”', '.', 'Paragraphs', 'tokenized', 'sentences', 'sentences', 'tokenized', 'words', '.', 'Given', 'character', 'sequence', 'defined', 'document', 'unit', ',', 'tokenization', 'task', 'chopping', 'pieces', ',', 'called', 'tokens', ',', 'perhaps', 'time', 'throwing', 'away', 'certain', 'characters', ',', 'punctuation', '.', 'Tokenization', 'essentially', 'splitting', 'phrase', ',', 'sentence', ',', 'paragraph', ',', 'entire', 'text', 'document', 'smaller', 'units', ',', 'individual', 'words', 'terms', '.', 'Each', 'smaller', 'units', 'called', 'tokens', '.']\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    if i not in stop_words:\n",
    "        after_filteration.append(i)\n",
    "        \n",
    "print(\"after filteration \\n\\n\",after_filteration)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b753227b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f90ad2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(after_filteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6090e88e",
   "metadata": {},
   "source": [
    "### Here we clearly see a diffrence between the number of chracters in WORDS & AFTER_FILTERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b21b853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "884"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e5e63ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'against', \"doesn't\", \"weren't\", \"hadn't\", 'only', \"shan't\", 'if', 'once', 'yourselves', 'has', 'out', 'from', 'wouldn', 'yours', 'off', 'was', 'doing', 'any', 'until', 'by', 'again', 'me', 'have', \"haven't\", 'didn', 'ma', 'but', 'at', 'with', 'why', 'mightn', 'themselves', \"mustn't\", 'above', 'while', \"don't\", 'hasn', 'to', \"didn't\", 'there', 'yourself', 'during', 'should', 'it', \"wasn't\", 'all', 're', 'o', \"mightn't\", 'were', 'being', 'they', 'are', 'do', 'that', 'him', 'these', \"you'll\", \"shouldn't\", 'its', 'for', 'such', \"isn't\", 'down', 'between', 'she', 'mustn', 'other', 'further', 'did', 'herself', \"you'd\", 'ourselves', 'on', 'so', 'a', 'in', 'very', 'couldn', 'don', 'isn', 'hadn', \"aren't\", 'now', 'whom', 'myself', 'an', 'hers', 'each', 'theirs', 'of', 'haven', 'needn', 'or', \"it's\", 'can', 'into', 'we', 'their', 'here', 'his', 'doesn', 'shan', 'own', 'as', 've', 'both', 'be', 'who', 'her', \"couldn't\", 'll', 'y', 'where', 'd', \"won't\", 'ours', \"should've\", \"you're\", 'what', 'same', 'am', 'which', 'about', \"she's\", 'itself', 'will', 'my', 'does', 'this', 'weren', \"hasn't\", 'because', 'few', 'through', 'most', 'too', 'you', 'than', 'shouldn', 'ain', 'before', \"you've\", 'nor', 't', 'just', 'having', 'the', 'after', 'up', 'below', 'himself', 'under', 'been', 'more', 'then', 'them', 'aren', 'had', \"needn't\", 'won', 'not', 'your', 'those', 'how', 'our', 'over', 'no', 'he', 'wasn', 'm', 'some', \"wouldn't\", 's', 'i', 'is', 'when', 'and', \"that'll\"}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b530b2ec",
   "metadata": {},
   "source": [
    "### These are the stops words present in English Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b17676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
